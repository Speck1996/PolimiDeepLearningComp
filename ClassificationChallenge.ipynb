{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7i_4ofsFTJt"
      },
      "source": [
        "# Image classification challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2I5GM-wFTJw"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%% random\n"
        },
        "trusted": true,
        "id": "s8jwCpWSFTJx"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from tensorflow_core.python.keras.layers.pooling import GlobalAveragePooling2D\n",
        "np.random.seed(4242)\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import os\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "trusted": true,
        "id": "cW6I8Y6zFTJy"
      },
      "outputs": [],
      "source": [
        "# Set the seed for random operations.\n",
        "# This let our experiments to be reproducible.\n",
        "SEED = 4242\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# Set GPU memory growth\n",
        "# Allows to only as much GPU memory as needed\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fob5TShKFTJy"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "trusted": true,
        "id": "bnePlmN0FTJz"
      },
      "outputs": [],
      "source": [
        "# ImageDataGenerator\n",
        "# ------------------\n",
        "apply_data_augmentation = True\n",
        "\n",
        "# Create training ImageDataGenerator object\n",
        "if apply_data_augmentation:\n",
        "    train_data_gen = ImageDataGenerator(rotation_range=10,\n",
        "                                        width_shift_range=0.15,\n",
        "                                        height_shift_range=0.15,\n",
        "                                        zoom_range=0.2,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=True,\n",
        "                                        fill_mode='constant',\n",
        "                                        rescale=1./255)\n",
        "else:\n",
        "    train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create validation ImageDataGenerator object\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "trusted": true,
        "id": "yU3Xc2VSFTJz"
      },
      "outputs": [],
      "source": [
        "# Create generators to read images from datasets\n",
        "# -------------------------------------------------------\n",
        "\n",
        "# Get current working directory\n",
        "#cwd = os.getcwd()\n",
        "cwd = '../input/'\n",
        "dataset_dir = os.path.join(cwd, 'Classification_Dataset')\n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "\n",
        "# batch size\n",
        "bs = 32\n",
        "# img shape\n",
        "img_h = 256\n",
        "img_w = 256\n",
        "\n",
        "num_classes = 20\n",
        "train_split = 0.8\n",
        "\n",
        "decide_class_indices = True\n",
        "if decide_class_indices:\n",
        "    classes = [\n",
        "        'owl',  # 0\n",
        "        'galaxy',  # 1\n",
        "        'lightning',  # 2\n",
        "        'wine-bottle',  # 3\n",
        "        't-shirt',  # 4\n",
        "        'waterfall',  # 5\n",
        "        'sword',  # 6\n",
        "        'school-bus',  # 7\n",
        "        'calculator',  # 8\n",
        "        'sheet-music',  # 9\n",
        "        'airplanes',  # 10\n",
        "        'lightbulb',  # 11\n",
        "        'skyscraper',  # 12\n",
        "        'mountain-bike',  # 13\n",
        "        'fireworks',  # 14\n",
        "        'computer-monitor',  # 15\n",
        "        'bear',  # 16\n",
        "        'grand-piano',  # 17\n",
        "        'kangaroo',  # 18\n",
        "        'laptop'  # 19\n",
        "        ]\n",
        "else:\n",
        "    classes = None\n",
        "\n",
        "\n",
        "def padding(image):\n",
        "    (height, width) = (image.shape[0], image.shape[1])\n",
        "    if width > height:\n",
        "        x = np.math.floor((width - height)/2)\n",
        "        y = np.math.ceil((width - height)/2)\n",
        "        return cv2.copyMakeBorder(image, x, y, 0, 0, cv2.BORDER_CONSTANT)\n",
        "    else:\n",
        "        x = np.math.floor((height - width)/2)\n",
        "        y = np.math.ceil((height - width)/2)\n",
        "        return cv2.copyMakeBorder(image, 0, 0, x, y, cv2.BORDER_CONSTANT)\n",
        "\n",
        "\n",
        "# split training and validation sets\n",
        "i = 0\n",
        "images = []\n",
        "for cls in classes:\n",
        "    cls_dir = os.path.join(training_dir, cls)\n",
        "    image_filenames = next(os.walk(cls_dir))[2]\n",
        "\n",
        "    # iterate through all the images in the training set\n",
        "    for img_name in image_filenames:\n",
        "        img = Image.open(os.path.join(cls_dir, img_name)).convert('RGB')\n",
        "        img_array = np.array(img)\n",
        "        img_array = cv2.resize(img_array, (img_h, img_w))\n",
        "        images.append(((img_array, img_name), i))\n",
        "    i = i + 1\n",
        "\n",
        "# shuffle to have random split\n",
        "np.random.shuffle(images)\n",
        "\n",
        "# split training and validation set\n",
        "train_set = images[:int(len(images)*train_split)]\n",
        "valid_set = images[int(len(images)*train_split):]\n",
        "\n",
        "# tranform sets into numpy array\n",
        "train_tuple_x, train_y = map(list, zip(*train_set))\n",
        "train_x, train_img_names = map(list, zip(*train_tuple_x))\n",
        "train_x = np.array(train_x)\n",
        "valid_tuple_x, valid_y = map(list, zip(*valid_set))\n",
        "valid_x, valid_img_names = map(list, zip(*valid_tuple_x))\n",
        "valid_x = np.array(valid_x)\n",
        "\n",
        "train_dict = {}\n",
        "valid_dict = {}\n",
        "\n",
        "for i in range(len(train_img_names)):\n",
        "    img_name = train_img_names[i]\n",
        "    class_x = classes[int(train_y[i])]\n",
        "    if class_x in train_dict:\n",
        "        train_dict[class_x].append(img_name)\n",
        "    else:\n",
        "        class_list = []\n",
        "        class_list.append(img_name)\n",
        "        train_dict[class_x] = class_list\n",
        "\n",
        "for i in range(len(valid_img_names)):\n",
        "    img_name = valid_img_names[i]\n",
        "    class_x = classes[int(valid_y[i])]\n",
        "    if class_x in valid_dict:\n",
        "        valid_dict[class_x].append(img_name)\n",
        "    else:\n",
        "        class_list = []\n",
        "        class_list.append(img_name)\n",
        "        valid_dict[class_x] = class_list\n",
        "\n",
        "final_dict = {\"training\": train_dict, \"validation\": valid_dict}\n",
        "with open('dataset_split.json', 'w') as json_file:\n",
        "    json.dump(final_dict, json_file)\n",
        "\n",
        "train_y = np.array(train_y)\n",
        "valid_y = np.array(valid_y)\n",
        "train_y = np_utils.to_categorical(train_y, num_classes)\n",
        "valid_y = np_utils.to_categorical(valid_y, num_classes)\n",
        "\n",
        "# Training\n",
        "train_gen = train_data_gen.flow(x=train_x,\n",
        "                                y=train_y,\n",
        "                                batch_size=bs,\n",
        "                                shuffle=True,\n",
        "                                seed=SEED)  # targets are directly converted into one-hot vectors\n",
        "\n",
        "# Validation\n",
        "valid_gen = valid_data_gen.flow(x=valid_x,\n",
        "                                y=valid_y,\n",
        "                                batch_size=bs,\n",
        "                                shuffle=False,\n",
        "                                seed=SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "trusted": true,
        "id": "YQaSkl4yFTJ1"
      },
      "outputs": [],
      "source": [
        "# Create Dataset objects\n",
        "# ----------------------\n",
        "\n",
        "# Training\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# Validation\n",
        "# ----------\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "valid_dataset = valid_dataset.repeat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOmNDzKhFTJ1"
      },
      "source": [
        "## Model Building\n",
        "CNN with Xception Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "trusted": true,
        "id": "sYbWBkncFTJ2"
      },
      "outputs": [],
      "source": [
        "transf_learn = False\n",
        "\n",
        "if transf_learn:\n",
        "    xcep = tf.keras.applications.Xception(\n",
        "        weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
        "    print('Model Loaded')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "trusted": true,
        "id": "Yk_N3dx3FTJ2"
      },
      "outputs": [],
      "source": [
        "if transf_learn:\n",
        "    xcep.summary()\n",
        "    xcep.layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "trusted": true,
        "id": "VHV2PlyWFTJ2"
      },
      "outputs": [],
      "source": [
        "from tensorflow_core.python.keras.layers.core import Dropout\n",
        "\n",
        "# Create Model With Transfer Learning\n",
        "# ------------\n",
        "\n",
        "if transf_learn:\n",
        "    finetuning = True\n",
        "\n",
        "    if finetuning:\n",
        "        freeze_until = 110  # layer from which we want to fine-tune\n",
        "\n",
        "        for layer in xcep.layers[:freeze_until]:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        xcep.trainable = False\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(xcep)\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dropout(0.25, seed=SEED))\n",
        "    model.add(tf.keras.layers.Dense(units=100, activation='sigmoid'))\n",
        "    model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "# Visualize created model as a table\n",
        "model.summary()\n",
        "\n",
        "# Visualize initialized weights\n",
        "model.weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "FiF64SZVFTJ3"
      },
      "source": [
        "Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "trusted": true,
        "id": "Kroczzd0FTJ3"
      },
      "outputs": [],
      "source": [
        "# Keras Model subclassing\n",
        "# -----------------------\n",
        "\n",
        "# Create convolutional block\n",
        "class ConvBlock(tf.keras.Model):\n",
        "    def __init__(self, num_filters):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv2d = tf.keras.layers.Conv2D(filters=num_filters,\n",
        "                                             kernel_size=(3, 3),\n",
        "                                             strides=(1, 1),\n",
        "                                             padding='valid')\n",
        "        # we can specify the activation function directly in Conv2D\n",
        "        self.activation = tf.keras.layers.ReLU()\n",
        "        self.pooling = tf.keras.layers.MaxPool2D(\n",
        "            pool_size=(2, 2), padding='same')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.conv2d(inputs)\n",
        "        x = self.activation(x)\n",
        "        x = self.pooling(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "trusted": true,
        "id": "-wxQP4yaFTJ3"
      },
      "outputs": [],
      "source": [
        "# Create Model Without Transfer Learning\n",
        "# ------------\n",
        "from tensorflow_core.python.keras.layers.core import Dropout\n",
        "\n",
        "\n",
        "class CNNClassifier(tf.keras.Model):\n",
        "    def __init__(self, depth, start_f, num_classes):\n",
        "        super(CNNClassifier, self).__init__()\n",
        "        \n",
        "        self.feature_extractor = tf.keras.Sequential()\n",
        "        for i in range(depth):\n",
        "            self.feature_extractor.add(ConvBlock(num_filters=start_f))\n",
        "            start_f *= 2\n",
        "            \n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.classifier = tf.keras.Sequential()\n",
        "        self.classifier.add(tf.keras.layers.Dense(units=512, activation='sigmoid'))\n",
        "        self.classifier.add(Dropout(0.25, seed=SEED))\n",
        "        self.classifier.add(tf.keras.layers.Dense(units=128, activation='sigmoid'))\n",
        "        self.classifier.add(Dropout(0.25, seed=SEED))\n",
        "        self.classifier.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.feature_extractor(inputs)\n",
        "        x = self.flatten(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "if not(transf_learn):\n",
        "    depth = 5\n",
        "    start_f = 7\n",
        "    num_classes = 20\n",
        "    # Create Model instance\n",
        "    model = CNNClassifier(depth=depth,\n",
        "                        start_f=start_f,\n",
        "                        num_classes=num_classes)\n",
        "    # Build Model\n",
        "    model.build(input_shape=(None, img_h, img_w, 3))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "trusted": true,
        "id": "N7XfNVsQFTJ3"
      },
      "outputs": [],
      "source": [
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "metrics = ['accuracy']\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1y1iz1ckFTJ4"
      },
      "source": [
        "## Training with callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "scrolled": true,
        "trusted": true,
        "id": "6BR4bVYyFTJ4"
      },
      "outputs": [],
      "source": [
        "out = '../output'\n",
        "exps_dir = os.path.join(out, 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'CNN'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "# ----------------\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "# ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'),\n",
        "#                                                   save_weights_only=True)  # False to save the model directly\n",
        "# callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "# ---------------------------------\n",
        "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "if not os.path.exists(tb_dir):\n",
        "    os.makedirs(tb_dir)\n",
        "    print(tb_dir)\n",
        "\n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=1)  # if 1 shows weights histograms\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=10)\n",
        "    callbacks.append(es_callback)\n",
        "\n",
        "\n",
        "model.fit(x=train_dataset,\n",
        "          epochs=100,\n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen),\n",
        "          callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRt5nig5FTJ4"
      },
      "source": [
        "## Test Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "trusted": true,
        "id": "2Wp5NiGCFTJ5"
      },
      "outputs": [],
      "source": [
        "def create_csv(results):\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(csv_fname, 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')\n",
        "\n",
        "\n",
        "image_filenames = next(os.walk('../input/Classification_Dataset/test'))[2]\n",
        "results = {}\n",
        "for img_name in image_filenames:\n",
        "\n",
        "    # processing the test image and getting the prediction\n",
        "    img = Image.open('../input/Classification_Dataset/test/' +\n",
        "                     img_name).convert('RGB')\n",
        "    img_array = np.array(img)\n",
        "    img_array = cv2.resize(img_array, (img_h, img_w))\n",
        "    img_array = np.expand_dims(img_array, 0)\n",
        "    img_array = img_array / 255.\n",
        "    prediction = np.argmax(model.predict(x=img_array))\n",
        "\n",
        "    results[img_name] = prediction\n",
        "\n",
        "create_csv(results)\n",
        "print(\"CSV done!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "CNNchallenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}